{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R2E Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a quickstart usage guide for R2E. It describes how to use the R2E's ([CLI](CLI.md)) to:\n",
    "1. setup and extract functions from repositories\n",
    "2. build and install repositories\n",
    "3. generate and execute **Equivalence Tests** for the functions\n",
    "\n",
    "Finally, it provides example use-cases of R2E such as building code generation benchmarks for LLMs and Agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setup and Extract\n",
    "\n",
    "First, choose a unique experiment id (e.g., `quickstart`) that you can reuse for the entire workflow. Then setup repositories and extract functions from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! r2e setup -r https://github.com/google-research/python-graphs\n",
    "! r2e extract -e quickstart --overwrite_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Build and Install\n",
    "\n",
    "**Docker Mode:** By default, all repos in REPOS_DIR are installed in a Docker image for sandboxed execution. Find the generated dockerfile in REPOS_DIR.\n",
    "\n",
    "**Local Mode:** Use `--local` which will suggest the steps ***you need to take to manually*** to install repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! r2e build -e quickstart --local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Generate and Execute Tests\n",
    "\n",
    "R2E provides a single command that runs a series of `k` generate-execute rounds w/ feedback. The loop continues until `min_valid`% functions reach a `min_cov`% branch coverage. Defaults: `k=3`, `min_valid=0.8`, and `min_cov=0.8`. \n",
    "\n",
    "Generated tests are executed in the Docker container. Use `--local` to execute locally.\n",
    "You can also run `r2e generate` and `r2e execute` separately ([cli.md](./CLI.md))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! r2e genexec -e quickstart --local --save_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! r2e show -e quickstart --summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of the functions extracted from the repositories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! r2e show -e quickstart -f cyclomatic_complexity --show-all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case: Coding Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from r2e.paths import *\n",
    "from r2e.utils.data import *\n",
    "from r2e.execution.helpers import run_fut_with_port, check_equiv\n",
    "from r2e.execution.service import ServiceManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a function you want to evaluate with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"make_node_from_ast_node\"\n",
    "futs = load_functions_under_test(EXECUTION_DIR / \"quickstart_out.json\")\n",
    "fut = next(f for f in futs if f.name == fname)\n",
    "# print(fut.context.context)  # uncomment to see the context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a codegen problem by removing the body of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_body(context, function_name):\n",
    "    import re\n",
    "\n",
    "    def replacement(match):\n",
    "        signature = match.group(1)\n",
    "        docstring = match.group(2) or \"\"\n",
    "        return f\"{signature}{docstring}pass\\n\"\n",
    "\n",
    "    pattern = rf'(def\\s+{re.escape(function_name)}\\s*\\([^)]*\\):)(\\s*\"\"\"[\\s\\S]*?\"\"\"\\s*)?([^@]+)'\n",
    "    modified_context = re.sub(pattern, replacement, context, flags=re.DOTALL)\n",
    "    return modified_context\n",
    "\n",
    "\n",
    "prob = remove_body(fut.context.context, fname)\n",
    "# print(prob) # uncomment to see the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate an LLM's ability to complete the function body (i.e., solve the problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from r2e.generators.testgen.utils import extract_codeblock\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_KEY\"))\n",
    "\n",
    "PROMPT = (\n",
    "    \"You are given a function signature, docstring, and some context.\\n\\n{prob}\\n\"\n",
    "    \"Complete the body of {fname} and return the entire function as\\n```python\\n# YOUR CODE HERE\\n```\"\n",
    ")\n",
    "\n",
    "def get_completion(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a Python programming expert.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return extract_codeblock(response.choices[0].message.content)\n",
    "\n",
    "code = get_completion(PROMPT.format(prob=prob, fname=fname))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run equivalence test to check if the generated code is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid, tb, res = check_equiv(code, fut, port=3006, local=True, reuse_port=True)\n",
    "\n",
    "print(\"Validity:\", valid)\n",
    "print(\"Traceback:\\n\", tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate **Agents** with execution feedback too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Agent(prob, fut, fname, max_attempts=5):\n",
    "    prompt = PROMPT.format(prob=prob, fname=fname)\n",
    "\n",
    "    for attempt in range(max_attempts):\n",
    "        code = get_completion(prompt)\n",
    "        print(f\"Attempt {attempt + 1}:\\n{code}\")\n",
    "        valid, tb, res = check_equiv(code, fut, port=3006, local=True, reuse_port=True)\n",
    "\n",
    "        if valid:\n",
    "            print(f\"✅ Solution found on attempt {attempt + 1}\\n\")\n",
    "            return json.dumps(res, indent=2)\n",
    "\n",
    "        print(f\"❌ Attempt {attempt + 1} failed. Refining based on feedback.\\n\")\n",
    "        prompt += f\"\\n\\nThe previous attempt failed.\\nHere's your previous attempt: {code}\\n\\nHere's the error:\\n{tb}\\nPlease fix the code and try again.\"\n",
    "\n",
    "    return f\"❌ Failed to find a valid solution after {max_attempts} attempts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = Agent(prob, fut, fname)\n",
    "print(f\"Result:\\n{res}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
